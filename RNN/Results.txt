# Predict 10 timestamps - 1K Epochs ....

kostakis@DESKTOP-HM9Q2U8:~/dev/Akis-Kostakis-Diploma-Thesis/RNN$ ./lstm.out -epochs 1000

LSTM Multi-Step Ahead Prediction
Configuration:
Lookback window (K): 10 timesteps
  Prediction horizon (N): 10 timesteps
  Input features: 9
  Output features: 3
  Epochs: 1000

Data/dataset_1.csv rows: 3397 Cols: 9 (3397x9)
Number of training samples: 3378 after removing windowSize and loopback
Converted eigen matrix to Tensor
Tensor shape: [3397, 9]
Angles tensor shape: [3397, 3]

Data Shapes
X (inputs):  [3378, 10, 9]
y (targets): [3378, 10, 3]

Model Created
Architecture: Input(9) -> LSTM(128) -> FC(30)

Training Configuration
Batch size: 32
Epochs: 1000
Learning rate: 0.001
Optimizer: Adam
Loss function: torch::nn::MSELossImpl

Starting Training
Epoch   0 | Loss: 0.010712
Epoch   5 | Loss: 0.002320
Epoch  10 | Loss: 0.001236
Epoch  15 | Loss: 0.000982
Epoch  20 | Loss: 0.000813
Epoch  25 | Loss: 0.000737
Epoch  30 | Loss: 0.000647
Epoch  35 | Loss: 0.000551
Epoch  40 | Loss: 0.000494
Epoch  45 | Loss: 0.000439
Epoch  50 | Loss: 0.000387
Epoch  55 | Loss: 0.000354
Epoch  60 | Loss: 0.000349
Epoch  65 | Loss: 0.000321
Epoch  70 | Loss: 0.000281
Epoch  75 | Loss: 0.000317
Epoch  80 | Loss: 0.000271
Epoch  85 | Loss: 0.000265
Epoch  90 | Loss: 0.000251
Epoch  95 | Loss: 0.000284
  -- Saved model: lstm_model_epoch_100.pt
Epoch 100 | Loss: 0.000214
Epoch 105 | Loss: 0.000301
Epoch 110 | Loss: 0.000212
Epoch 115 | Loss: 0.000230
Epoch 120 | Loss: 0.000201
Epoch 125 | Loss: 0.000234
Epoch 130 | Loss: 0.000220
Epoch 135 | Loss: 0.000203
Epoch 140 | Loss: 0.000215
Epoch 145 | Loss: 0.000190
Epoch 150 | Loss: 0.000218
Epoch 155 | Loss: 0.000223
Epoch 160 | Loss: 0.000203
Epoch 165 | Loss: 0.000194
Epoch 170 | Loss: 0.000230
Epoch 175 | Loss: 0.000201
Epoch 180 | Loss: 0.000218
Epoch 185 | Loss: 0.000255
Epoch 190 | Loss: 0.000261
Epoch 195 | Loss: 0.000228
  -- Saved model: lstm_model_epoch_200.pt
Epoch 200 | Loss: 0.000210
Epoch 205 | Loss: 0.000200
Epoch 210 | Loss: 0.000186
Epoch 215 | Loss: 0.000186
Epoch 220 | Loss: 0.000198
Epoch 225 | Loss: 0.000183
Epoch 230 | Loss: 0.000165
Epoch 235 | Loss: 0.000191
Epoch 240 | Loss: 0.000159
Epoch 245 | Loss: 0.000161
Epoch 250 | Loss: 0.000187
Epoch 255 | Loss: 0.000163
Epoch 260 | Loss: 0.000154
Epoch 265 | Loss: 0.000145
Epoch 270 | Loss: 0.000186
Epoch 275 | Loss: 0.000140
Epoch 280 | Loss: 0.000144
Epoch 285 | Loss: 0.000166
Epoch 290 | Loss: 0.000136
Epoch 295 | Loss: 0.000149
  -- Saved model: lstm_model_epoch_300.pt
Epoch 300 | Loss: 0.000133
Epoch 305 | Loss: 0.000142
Epoch 310 | Loss: 0.000130
Epoch 315 | Loss: 0.000134
Epoch 320 | Loss: 0.000129
Epoch 325 | Loss: 0.000128
Epoch 330 | Loss: 0.000146
Epoch 335 | Loss: 0.000114
Epoch 340 | Loss: 0.000128
Epoch 345 | Loss: 0.000105
Epoch 350 | Loss: 0.000120
Epoch 355 | Loss: 0.000125
Epoch 360 | Loss: 0.000141
Epoch 365 | Loss: 0.000115
Epoch 370 | Loss: 0.000102
Epoch 375 | Loss: 0.000112
Epoch 380 | Loss: 0.000095
Epoch 385 | Loss: 0.000111
Epoch 390 | Loss: 0.000102
Epoch 395 | Loss: 0.000104
  -- Saved model: lstm_model_epoch_400.pt
Epoch 400 | Loss: 0.000101
Epoch 405 | Loss: 0.000113
Epoch 410 | Loss: 0.000097
Epoch 415 | Loss: 0.000123
Epoch 420 | Loss: 0.000090
Epoch 425 | Loss: 0.000101
Epoch 430 | Loss: 0.000108
Epoch 435 | Loss: 0.000107
Epoch 440 | Loss: 0.000100
Epoch 445 | Loss: 0.000093
Epoch 450 | Loss: 0.000100
Epoch 455 | Loss: 0.000090
Epoch 460 | Loss: 0.000084
Epoch 465 | Loss: 0.000094
Epoch 470 | Loss: 0.000102
Epoch 475 | Loss: 0.000090
Epoch 480 | Loss: 0.000091
Epoch 485 | Loss: 0.000104
Epoch 490 | Loss: 0.000082
Epoch 495 | Loss: 0.000121
  -- Saved model: lstm_model_epoch_500.pt
Epoch 500 | Loss: 0.000096
Epoch 505 | Loss: 0.000077
Epoch 510 | Loss: 0.000088
Epoch 515 | Loss: 0.000105
Epoch 520 | Loss: 0.000078
Epoch 525 | Loss: 0.000094
Epoch 530 | Loss: 0.000108
Epoch 535 | Loss: 0.000091
Epoch 540 | Loss: 0.000104
Epoch 545 | Loss: 0.000078
Epoch 550 | Loss: 0.000076
Epoch 555 | Loss: 0.000095
Epoch 560 | Loss: 0.000115
Epoch 565 | Loss: 0.000094
Epoch 570 | Loss: 0.000064
Epoch 575 | Loss: 0.000073
Epoch 580 | Loss: 0.000112
Epoch 585 | Loss: 0.000073
Epoch 590 | Loss: 0.000065
Epoch 595 | Loss: 0.000078
  -- Saved model: lstm_model_epoch_600.pt
Epoch 600 | Loss: 0.000084
Epoch 605 | Loss: 0.000065
Epoch 610 | Loss: 0.000064
Epoch 615 | Loss: 0.000081
Epoch 620 | Loss: 0.000085
Epoch 625 | Loss: 0.000059
Epoch 630 | Loss: 0.000052
Epoch 635 | Loss: 0.000075
Epoch 640 | Loss: 0.000090
Epoch 645 | Loss: 0.000060
Epoch 650 | Loss: 0.000055
Epoch 655 | Loss: 0.000061
Epoch 660 | Loss: 0.000052
Epoch 665 | Loss: 0.000067
Epoch 670 | Loss: 0.000072
Epoch 675 | Loss: 0.000059
Epoch 680 | Loss: 0.000059
Epoch 685 | Loss: 0.000074
Epoch 690 | Loss: 0.000078
Epoch 695 | Loss: 0.000047
  -- Saved model: lstm_model_epoch_700.pt
Epoch 700 | Loss: 0.000072
Epoch 705 | Loss: 0.000050
Epoch 710 | Loss: 0.000065
Epoch 715 | Loss: 0.000063
Epoch 720 | Loss: 0.000075
Epoch 725 | Loss: 0.000047
Epoch 730 | Loss: 0.000066
Epoch 735 | Loss: 0.000047
Epoch 740 | Loss: 0.000068
Epoch 745 | Loss: 0.000072
Epoch 750 | Loss: 0.000050
Epoch 755 | Loss: 0.000058
Epoch 760 | Loss: 0.000077
Epoch 765 | Loss: 0.000050
Epoch 770 | Loss: 0.000043
Epoch 775 | Loss: 0.000079
Epoch 780 | Loss: 0.000045
Epoch 785 | Loss: 0.000043
Epoch 790 | Loss: 0.000057
Epoch 795 | Loss: 0.000061
  -- Saved model: lstm_model_epoch_800.pt
Epoch 800 | Loss: 0.000052
Epoch 805 | Loss: 0.000054
Epoch 810 | Loss: 0.000051
Epoch 815 | Loss: 0.000067
Epoch 820 | Loss: 0.000054
Epoch 825 | Loss: 0.000050
Epoch 830 | Loss: 0.000042
Epoch 835 | Loss: 0.000056
Epoch 840 | Loss: 0.000057
Epoch 845 | Loss: 0.000064
Epoch 850 | Loss: 0.000057
Epoch 855 | Loss: 0.000047
Epoch 860 | Loss: 0.000048
Epoch 865 | Loss: 0.000066
Epoch 870 | Loss: 0.000043
Epoch 875 | Loss: 0.000046
Epoch 880 | Loss: 0.000054
Epoch 885 | Loss: 0.000047
Epoch 890 | Loss: 0.000060
Epoch 895 | Loss: 0.000039
  -- Saved model: lstm_model_epoch_900.pt
Epoch 900 | Loss: 0.000050
Epoch 905 | Loss: 0.000044
Epoch 910 | Loss: 0.000049
Epoch 915 | Loss: 0.000035
Epoch 920 | Loss: 0.000065
Epoch 925 | Loss: 0.000039
Epoch 930 | Loss: 0.000041
Epoch 935 | Loss: 0.000062
Epoch 940 | Loss: 0.000048
Epoch 945 | Loss: 0.000043
Epoch 950 | Loss: 0.000040
Epoch 955 | Loss: 0.000045
Epoch 960 | Loss: 0.000055
Epoch 965 | Loss: 0.000044
Epoch 970 | Loss: 0.000054
Epoch 975 | Loss: 0.000040
Epoch 980 | Loss: 0.000051
Epoch 985 | Loss: 0.000045
Epoch 990 | Loss: 0.000051
Epoch 995 | Loss: 0.000043
Epoch 999 | Loss: 0.000044
  -- Saved model: lstm_model_epoch_1000.pt

Training Complete
Elapsed(s) = 645
Evaluating RMSE for each prediction step
Calculated all predictions size: [3378, 10, 3]
Overall Metrics:
  RMSE (all): 0.009754 rad = 0.559 deg
  MAE  (all): 0.007530 rad = 0.431 deg

RMSE per angle (all samples, all steps):
  Roll  RMSE: 0.012388 rad = 0.710 deg
  Pitch RMSE: 0.008085 rad = 0.463 deg
  Yaw   RMSE: 0.008160 rad = 0.468 deg

RMSE per step
Step | Roll (deg) | Pitch (deg) | Yaw (deg)
-----+------------+-------------+-----------
   1 |   0.671863 |    0.459779 |  0.363131
   2 |   0.634301 |    0.456966 |  0.375590
   3 |   0.632788 |    0.463662 |  0.430406
   4 |   0.649315 |    0.463163 |  0.502269
   5 |   0.676527 |    0.461990 |  0.528476
   6 |   0.706551 |    0.465251 |  0.525204
   7 |   0.737593 |    0.466595 |  0.501144
   8 |   0.763322 |    0.465659 |  0.464131
   9 |   0.784007 |    0.459088 |  0.460448
  10 |   0.814594 |    0.470284 |  0.491514


-----------------------------------------------------------------------






# Results

Epochs = 300
Overall Metrics:
  RMSE: 0.014321 rad = 0.821 deg
  MAE:  0.010296 rad = 0.590 deg

Training Complete
Elapsed(s) = 222
Evaluating RMSE for each prediction step
Calculated all predictions size: [3383, 5, 3]
Overall Metrics:
  RMSE (all): 0.012067 rad = 0.691 deg
  MAE  (all): 0.008878 rad = 0.509 deg

RMSE per angle (all samples, all steps):
  Roll  RMSE: 0.015381 rad = 0.881 deg
  Pitch RMSE: 0.011396 rad = 0.653 deg
  Yaw   RMSE: 0.008390 rad = 0.481 deg

RMSE per step
Step | Roll (deg) | Pitch (deg) | Yaw (deg)
-----+------------+-------------+-----------
   1 |   0.529896 |    0.475151 |  0.277213
   2 |   0.701064 |    0.426487 |  0.336128
   3 |   0.843295 |    0.504746 |  0.459770
   4 |   1.009834 |    0.748155 |  0.589079
   5 |   1.174697 |    0.953750 |  0.638116





# Results 1000 

kostakis@instance-20260102-140421:~/project$ docker run --rm lstm-app
LSTM Multi-Step Ahead Prediction
Configuration:
Lookback window (K): 10 timesteps
  Prediction horizon (N): 5 timesteps
  Input features: 9
  Output features: 3
  Epochs: 1000

Data/dataset_1.csv rows: 3397 Cols: 9 (3397x9)
Number of training samples: 3383 after removing windowSize and loopback
Converted eigen matrix to Tensor
Tensor shape: [3397, 9]
Angles tensor shape: [3397, 3]

Data Shapes
X (inputs):  [3383, 10, 9]
y (targets): [3383, 5, 3]

Model Created
Architecture: Input(9) -> LSTM(128) -> FC(15)

Training Configuration
Batch size: 32
Epochs: 1000
Learning rate: 0.001
Optimizer: Adam
Loss function: MSE

Starting Training
Epoch   0 | Loss: 0.010429
Epoch  10 | Loss: 0.000815
Epoch  20 | Loss: 0.000566
Epoch  30 | Loss: 0.000413
Epoch  40 | Loss: 0.000324
Epoch  50 | Loss: 0.000254
Epoch  60 | Loss: 0.000216
Epoch  70 | Loss: 0.000194
Epoch  80 | Loss: 0.000157
Epoch  90 | Loss: 0.000149
  -- Saved model: lstm_model_epoch_100.pt
Epoch 100 | Loss: 0.000130
Epoch 110 | Loss: 0.000123
Epoch 120 | Loss: 0.000126
Epoch 130 | Loss: 0.000097
Epoch 140 | Loss: 0.000113
Epoch 150 | Loss: 0.000094
Epoch 160 | Loss: 0.000119
Epoch 170 | Loss: 0.000104
Epoch 180 | Loss: 0.000090
Epoch 190 | Loss: 0.000092
  -- Saved model: lstm_model_epoch_200.pt
Epoch 200 | Loss: 0.000082
Epoch 210 | Loss: 0.000087
Epoch 220 | Loss: 0.000076
Epoch 230 | Loss: 0.000081
Epoch 240 | Loss: 0.000073
Epoch 250 | Loss: 0.000071
Epoch 260 | Loss: 0.000071
Epoch 270 | Loss: 0.000082
Epoch 280 | Loss: 0.000076
Epoch 290 | Loss: 0.000076
  -- Saved model: lstm_model_epoch_300.pt
Epoch 300 | Loss: 0.000071
Epoch 310 | Loss: 0.000078
Epoch 320 | Loss: 0.000079
Epoch 330 | Loss: 0.000121
Epoch 340 | Loss: 0.000120
Epoch 350 | Loss: 0.000070
Epoch 360 | Loss: 0.000088
Epoch 370 | Loss: 0.000071
Epoch 380 | Loss: 0.000065
Epoch 390 | Loss: 0.000069
  -- Saved model: lstm_model_epoch_400.pt
Epoch 400 | Loss: 0.000069
Epoch 410 | Loss: 0.000059
Epoch 420 | Loss: 0.000071
Epoch 430 | Loss: 0.000064
Epoch 440 | Loss: 0.000053
Epoch 450 | Loss: 0.000063
Epoch 460 | Loss: 0.000051
Epoch 470 | Loss: 0.000054
Epoch 480 | Loss: 0.000055
Epoch 490 | Loss: 0.000058
  -- Saved model: lstm_model_epoch_500.pt
Epoch 500 | Loss: 0.000052
Epoch 510 | Loss: 0.000051
Epoch 520 | Loss: 0.000042
Epoch 530 | Loss: 0.000045
Epoch 540 | Loss: 0.000048
Epoch 550 | Loss: 0.000063
Epoch 560 | Loss: 0.000039
Epoch 570 | Loss: 0.000040
Epoch 580 | Loss: 0.000069
Epoch 590 | Loss: 0.000038
  -- Saved model: lstm_model_epoch_600.pt
Epoch 600 | Loss: 0.000043
Epoch 610 | Loss: 0.000034
Epoch 620 | Loss: 0.000045
Epoch 630 | Loss: 0.000040
Epoch 640 | Loss: 0.000049
Epoch 650 | Loss: 0.000035
Epoch 660 | Loss: 0.000045
Epoch 670 | Loss: 0.000040
Epoch 680 | Loss: 0.000040
Epoch 690 | Loss: 0.000038
  -- Saved model: lstm_model_epoch_700.pt
Epoch 700 | Loss: 0.000036
Epoch 710 | Loss: 0.000049
Epoch 720 | Loss: 0.000050
Epoch 730 | Loss: 0.000042
Epoch 740 | Loss: 0.000053
Epoch 750 | Loss: 0.000043
Epoch 760 | Loss: 0.000060
Epoch 770 | Loss: 0.000041
Epoch 780 | Loss: 0.000039
Epoch 790 | Loss: 0.000033
  -- Saved model: lstm_model_epoch_800.pt
Epoch 800 | Loss: 0.000037
Epoch 810 | Loss: 0.000031
Epoch 820 | Loss: 0.000053
Epoch 830 | Loss: 0.000036
Epoch 840 | Loss: 0.000036
Epoch 850 | Loss: 0.000035
Epoch 860 | Loss: 0.000033
Epoch 870 | Loss: 0.000038
Epoch 880 | Loss: 0.000044
Epoch 890 | Loss: 0.000029
  -- Saved model: lstm_model_epoch_900.pt
Epoch 900 | Loss: 0.000031
Epoch 910 | Loss: 0.000037
Epoch 920 | Loss: 0.000040
Epoch 930 | Loss: 0.000031
Epoch 940 | Loss: 0.000033
Epoch 950 | Loss: 0.000032
Epoch 960 | Loss: 0.000028
Epoch 970 | Loss: 0.000023
Epoch 980 | Loss: 0.000043
Epoch 990 | Loss: 0.000037
Epoch 999 | Loss: 0.000031
  -- Saved model: lstm_model_epoch_1000.pt

Training Complete
Elapsed(s) = 975
Evaluating RMSE for each prediction step
Calculated all predictions size: [3383, 5, 3]
Overall Metrics:
  RMSE (all): 0.008412 rad = 0.482 deg
  MAE  (all): 0.006631 rad = 0.380 deg

RMSE per angle (all samples, all steps):
  Roll  RMSE: 0.009369 rad = 0.537 deg
  Pitch RMSE: 0.007870 rad = 0.451 deg
  Yaw   RMSE: 0.007911 rad = 0.453 deg

RMSE per step
Step | Roll (deg) | Pitch (deg) | Yaw (deg)
-----+------------+-------------+-----------
   1 |   0.469796 |    0.256466 |  0.368581
   2 |   0.474055 |    0.355238 |  0.343172
   3 |   0.535472 |    0.437338 |  0.415016
   4 |   0.570142 |    0.527777 |  0.569398
   5 |   0.619205 |    0.595730 |  0.526403




# Results 3000

kostakis@instance-20260102-140421:~/project$ docker run --rm lstm-app
LSTM Multi-Step Ahead Prediction
Configuration:
Lookback window (K): 10 timesteps
  Prediction horizon (N): 5 timesteps
  Input features: 9
  Output features: 3
  Epochs: 3000

Data/dataset_1.csv rows: 3397 Cols: 9 (3397x9)
Number of training samples: 3383 after removing windowSize and loopback
Converted eigen matrix to Tensor
Tensor shape: [3397, 9]
Angles tensor shape: [3397, 3]

Data Shapes
X (inputs):  [3383, 10, 9]
y (targets): [3383, 5, 3]

Model Created
Architecture: Input(9) -> LSTM(128) -> FC(15)

Training Configuration
Batch size: 32
Epochs: 3000
Learning rate: 0.001
Optimizer: Adam
Loss function: MSE

Starting Training
Epoch   0 | Loss: 0.010813
Epoch  10 | Loss: 0.000818
Epoch  20 | Loss: 0.000553
Epoch  30 | Loss: 0.000406
Epoch  40 | Loss: 0.000320
Epoch  50 | Loss: 0.000284
Epoch  60 | Loss: 0.000236
Epoch  70 | Loss: 0.000173
Epoch  80 | Loss: 0.000190
Epoch  90 | Loss: 0.000156
  -- Saved model: lstm_model_epoch_100.pt
Epoch 100 | Loss: 0.000130
Epoch 110 | Loss: 0.000116
Epoch 120 | Loss: 0.000135
Epoch 130 | Loss: 0.000128
Epoch 140 | Loss: 0.000125
Epoch 150 | Loss: 0.000103
Epoch 160 | Loss: 0.000089
Epoch 170 | Loss: 0.000102
Epoch 180 | Loss: 0.000099
Epoch 190 | Loss: 0.000093
  -- Saved model: lstm_model_epoch_200.pt
Epoch 200 | Loss: 0.000094
Epoch 210 | Loss: 0.000079
Epoch 220 | Loss: 0.000084
Epoch 230 | Loss: 0.000084
Epoch 240 | Loss: 0.000079
Epoch 250 | Loss: 0.000080
Epoch 260 | Loss: 0.000073
Epoch 270 | Loss: 0.000064
Epoch 280 | Loss: 0.000095
Epoch 290 | Loss: 0.000063
  -- Saved model: lstm_model_epoch_300.pt
Epoch 300 | Loss: 0.000082
Epoch 310 | Loss: 0.000067
Epoch 320 | Loss: 0.000067
Epoch 330 | Loss: 0.000087
Epoch 340 | Loss: 0.000092
Epoch 350 | Loss: 0.000110
Epoch 360 | Loss: 0.000066
Epoch 370 | Loss: 0.000069
Epoch 380 | Loss: 0.000083
Epoch 390 | Loss: 0.000068
  -- Saved model: lstm_model_epoch_400.pt
Epoch 400 | Loss: 0.000087
Epoch 410 | Loss: 0.000055
Epoch 420 | Loss: 0.000065
Epoch 430 | Loss: 0.000055
Epoch 440 | Loss: 0.000053
Epoch 450 | Loss: 0.000061
Epoch 460 | Loss: 0.000058
Epoch 470 | Loss: 0.000051
Epoch 480 | Loss: 0.000045
Epoch 490 | Loss: 0.000063
  -- Saved model: lstm_model_epoch_500.pt
Epoch 500 | Loss: 0.000041
Epoch 510 | Loss: 0.000046
Epoch 520 | Loss: 0.000051
Epoch 530 | Loss: 0.000042
Epoch 540 | Loss: 0.000042
Epoch 550 | Loss: 0.000046
Epoch 560 | Loss: 0.000039
Epoch 570 | Loss: 0.000049
Epoch 580 | Loss: 0.000042
Epoch 590 | Loss: 0.000050
  -- Saved model: lstm_model_epoch_600.pt
Epoch 600 | Loss: 0.000041
Epoch 610 | Loss: 0.000037
Epoch 620 | Loss: 0.000045
Epoch 630 | Loss: 0.000046
Epoch 640 | Loss: 0.000038
Epoch 650 | Loss: 0.000051
Epoch 660 | Loss: 0.000045
Epoch 670 | Loss: 0.000038
Epoch 680 | Loss: 0.000048
Epoch 690 | Loss: 0.000036
  -- Saved model: lstm_model_epoch_700.pt
Epoch 700 | Loss: 0.000045
Epoch 710 | Loss: 0.000040
Epoch 720 | Loss: 0.000038
Epoch 730 | Loss: 0.000029
Epoch 740 | Loss: 0.000031
Epoch 750 | Loss: 0.000031
Epoch 760 | Loss: 0.000029
Epoch 770 | Loss: 0.000030
Epoch 780 | Loss: 0.000032
Epoch 790 | Loss: 0.000040
  -- Saved model: lstm_model_epoch_800.pt
Epoch 800 | Loss: 0.000029
Epoch 810 | Loss: 0.000039
Epoch 820 | Loss: 0.000031
Epoch 830 | Loss: 0.000034
Epoch 840 | Loss: 0.000040
Epoch 850 | Loss: 0.000032
Epoch 860 | Loss: 0.000033
Epoch 870 | Loss: 0.000030
Epoch 880 | Loss: 0.000030
Epoch 890 | Loss: 0.000040
  -- Saved model: lstm_model_epoch_900.pt
Epoch 900 | Loss: 0.000028
Epoch 910 | Loss: 0.000032
Epoch 920 | Loss: 0.000028
Epoch 930 | Loss: 0.000032
Epoch 940 | Loss: 0.000035
Epoch 950 | Loss: 0.000033
Epoch 960 | Loss: 0.000033
Epoch 970 | Loss: 0.000025
Epoch 980 | Loss: 0.000027
Epoch 990 | Loss: 0.000028
  -- Saved model: lstm_model_epoch_1000.pt
Epoch 1000 | Loss: 0.000029
Epoch 1010 | Loss: 0.000033
Epoch 1020 | Loss: 0.000031
Epoch 1030 | Loss: 0.000032
Epoch 1040 | Loss: 0.000025
Epoch 1050 | Loss: 0.000028
Epoch 1060 | Loss: 0.000023
Epoch 1070 | Loss: 0.000028
Epoch 1080 | Loss: 0.000022
Epoch 1090 | Loss: 0.000023
  -- Saved model: lstm_model_epoch_1100.pt
Epoch 1100 | Loss: 0.000033
Epoch 1110 | Loss: 0.000019
Epoch 1120 | Loss: 0.000025
Epoch 1130 | Loss: 0.000020
Epoch 1140 | Loss: 0.000024
Epoch 1150 | Loss: 0.000027
Epoch 1160 | Loss: 0.000020
Epoch 1170 | Loss: 0.000022
Epoch 1180 | Loss: 0.000025
Epoch 1190 | Loss: 0.000020
  -- Saved model: lstm_model_epoch_1200.pt
Epoch 1200 | Loss: 0.000039
Epoch 1210 | Loss: 0.000025
Epoch 1220 | Loss: 0.000039
Epoch 1230 | Loss: 0.000019
Epoch 1240 | Loss: 0.000019
Epoch 1250 | Loss: 0.000034
Epoch 1260 | Loss: 0.000027
Epoch 1270 | Loss: 0.000021
Epoch 1280 | Loss: 0.000024
Epoch 1290 | Loss: 0.000022
  -- Saved model: lstm_model_epoch_1300.pt
Epoch 1300 | Loss: 0.000020
Epoch 1310 | Loss: 0.000023
Epoch 1320 | Loss: 0.000021
Epoch 1330 | Loss: 0.000023
Epoch 1340 | Loss: 0.000024
Epoch 1350 | Loss: 0.000031
Epoch 1360 | Loss: 0.000022
Epoch 1370 | Loss: 0.000023
Epoch 1380 | Loss: 0.000018
Epoch 1390 | Loss: 0.000027
  -- Saved model: lstm_model_epoch_1400.pt
Epoch 1400 | Loss: 0.000024
Epoch 1410 | Loss: 0.000032
Epoch 1420 | Loss: 0.000023
Epoch 1430 | Loss: 0.000024
Epoch 1440 | Loss: 0.000021
Epoch 1450 | Loss: 0.000026
Epoch 1460 | Loss: 0.000025
Epoch 1470 | Loss: 0.000021
Epoch 1480 | Loss: 0.000032
Epoch 1490 | Loss: 0.000025
  -- Saved model: lstm_model_epoch_1500.pt
Epoch 1500 | Loss: 0.000027
Epoch 1510 | Loss: 0.000041
Epoch 1520 | Loss: 0.000027
Epoch 1530 | Loss: 0.000023
Epoch 1540 | Loss: 0.000038
Epoch 1550 | Loss: 0.000031
Epoch 1560 | Loss: 0.000026
Epoch 1570 | Loss: 0.000034
Epoch 1580 | Loss: 0.000027
Epoch 1590 | Loss: 0.000035
  -- Saved model: lstm_model_epoch_1600.pt
Epoch 1600 | Loss: 0.000030
Epoch 1610 | Loss: 0.000033
Epoch 1620 | Loss: 0.000028
Epoch 1630 | Loss: 0.000033
Epoch 1640 | Loss: 0.000026
Epoch 1650 | Loss: 0.000018
Epoch 1660 | Loss: 0.000021
Epoch 1670 | Loss: 0.000025
Epoch 1680 | Loss: 0.000023
Epoch 1690 | Loss: 0.000025
  -- Saved model: lstm_model_epoch_1700.pt
Epoch 1700 | Loss: 0.000020
Epoch 1710 | Loss: 0.000021
Epoch 1720 | Loss: 0.000017
Epoch 1730 | Loss: 0.000022
Epoch 1740 | Loss: 0.000017
Epoch 1750 | Loss: 0.000027
Epoch 1760 | Loss: 0.000019
Epoch 1770 | Loss: 0.000027
Epoch 1780 | Loss: 0.000017
Epoch 1790 | Loss: 0.000021
  -- Saved model: lstm_model_epoch_1800.pt
Epoch 1800 | Loss: 0.000023
Epoch 1810 | Loss: 0.000021
Epoch 1820 | Loss: 0.000024
Epoch 1830 | Loss: 0.000016
Epoch 1840 | Loss: 0.000020
Epoch 1850 | Loss: 0.000021
Epoch 1860 | Loss: 0.000016
Epoch 1870 | Loss: 0.000020
Epoch 1880 | Loss: 0.000019
Epoch 1890 | Loss: 0.000019
  -- Saved model: lstm_model_epoch_1900.pt
Epoch 1900 | Loss: 0.000015
Epoch 1910 | Loss: 0.000025
Epoch 1920 | Loss: 0.000020
Epoch 1930 | Loss: 0.000024
Epoch 1940 | Loss: 0.000025
Epoch 1950 | Loss: 0.000024
Epoch 1960 | Loss: 0.000018
Epoch 1970 | Loss: 0.000022
Epoch 1980 | Loss: 0.000022
Epoch 1990 | Loss: 0.000036
  -- Saved model: lstm_model_epoch_2000.pt
Epoch 2000 | Loss: 0.000033
Epoch 2010 | Loss: 0.000024
Epoch 2020 | Loss: 0.000022
Epoch 2030 | Loss: 0.000021
Epoch 2040 | Loss: 0.000018
Epoch 2050 | Loss: 0.000022
Epoch 2060 | Loss: 0.000025
Epoch 2070 | Loss: 0.000029
Epoch 2080 | Loss: 0.000017
Epoch 2090 | Loss: 0.000020
  -- Saved model: lstm_model_epoch_2100.pt
Epoch 2100 | Loss: 0.000020
Epoch 2110 | Loss: 0.000021
Epoch 2120 | Loss: 0.000026
Epoch 2130 | Loss: 0.000025
Epoch 2140 | Loss: 0.000021
Epoch 2150 | Loss: 0.000020
Epoch 2160 | Loss: 0.000022
Epoch 2170 | Loss: 0.000022
Epoch 2180 | Loss: 0.000017
Epoch 2190 | Loss: 0.000019
  -- Saved model: lstm_model_epoch_2200.pt
Epoch 2200 | Loss: 0.000027
Epoch 2210 | Loss: 0.000024
Epoch 2220 | Loss: 0.000021
Epoch 2230 | Loss: 0.000032
Epoch 2240 | Loss: 0.000015
Epoch 2250 | Loss: 0.000033
Epoch 2260 | Loss: 0.000021
Epoch 2270 | Loss: 0.000030
Epoch 2280 | Loss: 0.000033
Epoch 2290 | Loss: 0.000027
  -- Saved model: lstm_model_epoch_2300.pt
Epoch 2300 | Loss: 0.000028
Epoch 2310 | Loss: 0.000028
Epoch 2320 | Loss: 0.000023
Epoch 2330 | Loss: 0.000027
Epoch 2340 | Loss: 0.000019
Epoch 2350 | Loss: 0.000019
Epoch 2360 | Loss: 0.000014
Epoch 2370 | Loss: 0.000017
Epoch 2380 | Loss: 0.000017
Epoch 2390 | Loss: 0.000015
  -- Saved model: lstm_model_epoch_2400.pt
Epoch 2400 | Loss: 0.000019
Epoch 2410 | Loss: 0.000017
Epoch 2420 | Loss: 0.000011
Epoch 2430 | Loss: 0.000019
Epoch 2440 | Loss: 0.000018
Epoch 2450 | Loss: 0.000012
Epoch 2460 | Loss: 0.000022
Epoch 2470 | Loss: 0.000016
Epoch 2480 | Loss: 0.000016
Epoch 2490 | Loss: 0.000017
  -- Saved model: lstm_model_epoch_2500.pt
Epoch 2500 | Loss: 0.000016
Epoch 2510 | Loss: 0.000024
Epoch 2520 | Loss: 0.000024
Epoch 2530 | Loss: 0.000027
Epoch 2540 | Loss: 0.000024
Epoch 2550 | Loss: 0.000024
Epoch 2560 | Loss: 0.000023
Epoch 2570 | Loss: 0.000021
Epoch 2580 | Loss: 0.000019
Epoch 2590 | Loss: 0.000018
  -- Saved model: lstm_model_epoch_2600.pt
Epoch 2600 | Loss: 0.000016
Epoch 2610 | Loss: 0.000017
Epoch 2620 | Loss: 0.000018
Epoch 2630 | Loss: 0.000015
Epoch 2640 | Loss: 0.000018
Epoch 2650 | Loss: 0.000015
Epoch 2660 | Loss: 0.000019
Epoch 2670 | Loss: 0.000017
Epoch 2680 | Loss: 0.000024
Epoch 2690 | Loss: 0.000017
  -- Saved model: lstm_model_epoch_2700.pt
Epoch 2700 | Loss: 0.000017
Epoch 2710 | Loss: 0.000035
Epoch 2720 | Loss: 0.000034
Epoch 2730 | Loss: 0.000034
Epoch 2740 | Loss: 0.000031
Epoch 2750 | Loss: 0.000027
Epoch 2760 | Loss: 0.000030
Epoch 2770 | Loss: 0.000019
Epoch 2780 | Loss: 0.000018
Epoch 2790 | Loss: 0.000018
  -- Saved model: lstm_model_epoch_2800.pt
Epoch 2800 | Loss: 0.000015
Epoch 2810 | Loss: 0.000015
Epoch 2820 | Loss: 0.000021
Epoch 2830 | Loss: 0.000017
Epoch 2840 | Loss: 0.000023
Epoch 2850 | Loss: 0.000028
Epoch 2860 | Loss: 0.000032
Epoch 2870 | Loss: 0.000030
Epoch 2880 | Loss: 0.000027
Epoch 2890 | Loss: 0.000021
  -- Saved model: lstm_model_epoch_2900.pt
Epoch 2900 | Loss: 0.000019
Epoch 2910 | Loss: 0.000020
Epoch 2920 | Loss: 0.000018
Epoch 2930 | Loss: 0.000014
Epoch 2940 | Loss: 0.000027
Epoch 2950 | Loss: 0.000024
Epoch 2960 | Loss: 0.000029
Epoch 2970 | Loss: 0.000022
Epoch 2980 | Loss: 0.000020
Epoch 2990 | Loss: 0.000017
Epoch 2999 | Loss: 0.000016
  -- Saved model: lstm_model_epoch_3000.pt

Training Complete
Elapsed(s) = 3010
Evaluating RMSE for each prediction step
Calculated all predictions size: [3383, 5, 3]
Overall Metrics:
  RMSE (all): 0.005784 rad = 0.331 deg
  MAE  (all): 0.004226 rad = 0.242 deg

RMSE per angle (all samples, all steps):
  Roll  RMSE: 0.007182 rad = 0.411 deg
  Pitch RMSE: 0.006081 rad = 0.348 deg
  Yaw   RMSE: 0.003435 rad = 0.197 deg

RMSE per step
Step | Roll (deg) | Pitch (deg) | Yaw (deg)
-----+------------+-------------+-----------
   1 |   0.342970 |    0.252574 |  0.133922
   2 |   0.369070 |    0.264470 |  0.149232
   3 |   0.375247 |    0.297349 |  0.175275
   4 |   0.453891 |    0.401733 |  0.216613
   5 |   0.495949 |    0.472658 |  0.275322